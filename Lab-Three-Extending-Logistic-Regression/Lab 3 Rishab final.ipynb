{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Three: Extending Logistic Regression\n",
    "\n",
    "## Team Members\n",
    "1. Piyush Metkar  - 47509180\n",
    "2. Rishab Vaishya - 47505527\n",
    "3. Dhaval Gogri   - 47444609\n",
    "\n",
    "\n",
    "# 1. PREPARATION AND OVERVIEW\n",
    "\n",
    "## 1.1 Business Understanding\n",
    "\n",
    "The dataset **Grade Prediction** is taken from the Professor Moody Prediction dataset and it is about the behaviour of students and their grades they received in his class. Professor Moody observed various behavioural aspects of his students such as how much involvement they have in the class, how punctual are they and how much focus they had in the class. He quantified these aspects as 'never', 'rarely' and 'always'. He also added their grades to observe what impact their behaviour had on them. The dataset has a file with around 1000 instances.\n",
    "\n",
    "The source of the dataset is as follows: https://www.kaggle.com/c/grade-prediction\n",
    "\n",
    "The aim of this project is to predict the academic performance of students and **classify** them as per grades depending on their behaviour in class. The target class of this dataset is **Grades** column which is categorized in values A, B, C, D and F. \n",
    "\n",
    "We all know how much important the role of education is in the society. Analyzing some of the behavioral patterns of a student can help us gain an insight on their **academic performance of the future**. This information can be useful for faculty members to identify student which needs extra help with the course. It can be useful for parents who can predict how well their child is expected to perform in their academics. Students themseleves can also analyze their behavior and implement changes in order to increase their success chances.\n",
    "\n",
    "On a broader level, It is useful for the university to know how their students are likely to behave & their competance against students of other universities. Since education is a vital part of the society in order to successfully mould the next generation of people. \n",
    "\n",
    "Many curious questions can be answered like reason for a students poor performance even after 100% attandance in the class. Percentage of people who manage to score good grades even after bunking a lot of lectures. Why do people score less even after clearing their doubts from professor. The results may not be what we generally assume.\n",
    "\n",
    "Assuming human behaviour is more or less consistent in majority of societies & times. These predictions can be scaled to different regions and also can be used in the coming future.\n",
    "\n",
    "## 1.2 Data Preparation & Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 2] No such file or directory: '/Users/piyushmetkar/Documents/SMU/Notes/Spring 18/7324 Machine Learning in Python/Lab 3\\\\moody_training_data.csv'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen_local_file\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/piyushmetkar/Documents/SMU/Notes/Spring 18/7324 Machine Learning in Python/Lab 3\\\\moody_training_data.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0404e928dae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'file:///Users/piyushmetkar/Documents/SMU/Notes/Spring 18/7324 Machine Learning in Python/Lab 3\\moody_training_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[0;32m--> 433\u001b[0;31m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gzip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mfile_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1450\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:// scheme is supported only on localhost\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_local_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;31m# names for the localhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen_local_file\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1489\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0maddinfourl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigurl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file not on local host'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 2] No such file or directory: '/Users/piyushmetkar/Documents/SMU/Notes/Spring 18/7324 Machine Learning in Python/Lab 3\\\\moody_training_data.csv'>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r'D:\\Downloads\\Notes\\CSE_7324_ML\\Lab 3\\moody_training_data.csv') \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the CSV data file into pandas dataframe object. We will extract the values of columns from df into numpy arrays required for classification.\n",
    "\n",
    "The grades column is categorized as A, B, C, D and F as per the value of scores. We don't need the exact values of thier scores rather we are interested in grades. Hence we do not include the Score and StudentID columns.\n",
    "\n",
    "The behaviour of the students such as do they use phones, do they ask questions and are they punctual are quantified in words 'never', 'sometimes', 'rarely', 'frequently' and 'always'. Based on their meaning, we can compare the words as follows since they are **ordinal**:\n",
    "\n",
    "never < sometimes/rarely < frequently < always\n",
    "\n",
    "Hence we replace these values with integers. \n",
    "\n",
    "We also replace all the grades in target class which is also of type **ordinal** with integers. Grades A, B, C, D and F are replaced by 0, 1, 2, 3 and 4 repectively. This will also help us to compare the grade.\n",
    "\n",
    "Our dataset has **only 3 features** and hence we **don't need** to perform any **dimensionality reduction** technique on it. Also our data doesn't have any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ASKSQUESTIONS'].replace(['never', 'rarely', 'sometimes', 'frequently', 'always'], [0, 1, 1, 2, 3],inplace=True)\n",
    "df['TEXTINGINCLASS'].replace(['never', 'rarely', 'sometimes', 'frequently', 'always'], [0, 1, 1, 2, 3],inplace=True)\n",
    "df['LATEINCLASS'].replace(['never', 'rarely', 'sometimes', 'frequently', 'always'], [0, 1, 1, 2, 3],inplace=True)\n",
    "df['GRADE'].replace(['A', 'B', 'C', 'D', 'F'], [0, 1, 2, 3, 4],inplace=True)\n",
    "\n",
    "#ref : https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n",
    "X = df.iloc[:, [3, 4, 5]].values\n",
    "y = df['GRADE'].values\n",
    "class_names = np.asarray(['A', 'B', 'C', 'D', 'F']) \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset contains 3 columns ASKSQUESTIONS, TEXTINGINCLASS and LATEINCLASS and have removed redundant columns. We have replaced all the ordinal values with integers. There are around 1000 instances present in the data.\n",
    "\n",
    "## 1.3 Data division\n",
    "Let's divide the data in **80:20** ratio, 80% being training data and 20% being test data.\n",
    "\n",
    "### Cross Validation\n",
    "However, in order to divide the data, we need to make sure that the results are not biased in any cluster and it is consistent throughout the dataset. We are using **K Fold Cross-Validation** technique to verify the consistency in data.\n",
    "\n",
    "The following algorithm works by dividing the dataset in **K=5** parts. For each iteration, 1 part becomes the test set and the remaining 4 parts become the train set. We create a new model for each iteration by using these 4 parts of the train set and thus verify it against the current test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#ref: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "kf = KFold(n_splits=5) # Define the split - into 5 folds\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "sum = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    Xk_train, Xk_test = X[train_index], X[test_index]\n",
    "    yk_train, yk_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = lm.fit(Xk_train, yk_train)\n",
    "    yk_hat = lm.predict(Xk_test)\n",
    "    acc = accuracy_score(yk_test,yk_hat.astype(int))\n",
    "    print('Accuracy : ', acc)\n",
    "    sum += acc \n",
    "\n",
    "print('\\nAverage :', sum/5)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all the iterations have very similar accuracy. This proves that the data is consistent throughout and hence we can split the data in to 80:20 ratio for test purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ref: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODELING\n",
    "\n",
    "## 2.1 Implementing Logistic Regression\n",
    "\n",
    "Now that we have cleansed and prepared our dataset, It is ready for implementing Logistic regression. In the following code, We have created many types of binary logistic regression class which will be used in their multi-class regression implementation.\n",
    "\n",
    "Source for the follwing code :- https://github.com/eclarson/MachineLearningNotebooks/blob/master/05.%20Logistic%20Regression.ipynb\n",
    "\n",
    "## Vector Binary Logistic Regression\n",
    "\n",
    "This class for vector Binary logistic regression is defined below.\n",
    "\n",
    "It has the following function :\n",
    "1. private init\n",
    "2. static add_bias\n",
    "3. public predict_proba\n",
    "4. predict\n",
    "5. private str\n",
    "6. public fit\n",
    "7. static sigmoid - this using the expit function which is imported in this block\n",
    "8. get_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class VectorBinaryLogisticRegression:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    " \n",
    "     #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "            \n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        return gradient.reshape(self.w_.shape)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Logistic Regression using Vector Binary Logistic Regression\n",
    "\n",
    "Here in LogisticRegression class defined below, we are using the Vector Logistic Regression class which are used for binary class and running it for each unique classes in the datasets (which in our case is the grades) in order to implement it for multi-class using **one-versus-all** method.\n",
    "\n",
    "It has the following functions :\n",
    "1. private init\n",
    "2. private str\n",
    "3. public fit\n",
    "4. public predict_proba\n",
    "5. predict\n",
    "\n",
    "we are creating the object of this class at the bottom of this block and put our dataset into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n",
    "lr = LogisticRegression(0.1,100)\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we see that Multi-class Logistic Regression using Vector Binary Logistic Regression took 32.3 Seconds to execute and gives us an accuracy of 43.68 %\n",
    "\n",
    "## 2.1.1 Optimized Logistic Regression\n",
    "\n",
    "The following class will be used to optimize logistic regression and use it for datasets with multi unique classes.\n",
    "\n",
    "Source for the follwing code :- https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb\n",
    " \n",
    "## Binary Logistic Regression Base Class\n",
    " \n",
    "This is the **base class** for all the following classes which will help us in optimizing logistic regression implementation\n",
    " \n",
    "It has the following function :\n",
    "1. private init\n",
    "2. private str\n",
    "3. static add_bias\n",
    "4. static sigmoid - this using the expit function which is imported in this block\n",
    "5. get_gradient\n",
    "6. public predict_proba\n",
    "7. predict\n",
    "8. public fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Search Logistic Regression\n",
    "\n",
    "This class defines the optimization technique we can use for logistic regression using **Line search**. This class is designed to work for datasets having binary classes. This class will be further used for datasets with multi-class\n",
    "\n",
    "As this class inherits most of it's features from **BinaryLogisticRegression** class\n",
    "We only modify what's needed to optimize it using line search.\n",
    "\n",
    "Fit is the only modified functions which now implements line search function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad,C):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)-C*np.sum(wnew**2)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Logistic Regression\n",
    "\n",
    "This class defines the optimization technique we can use for logistic regression using **Stochastic** method. This class is designed to work for datasets having binary classes. This class will be further used for datasets with multi-class\n",
    "\n",
    "As this class inherits most of it's features from **BinaryLogisticRegression** class\n",
    "We only modify what's needed to optimize it using Stochastic method.\n",
    "\n",
    "Gradient is modified in this method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Binary Logistic Regression\n",
    "\n",
    "\n",
    "This class defines the optimization technique we can use for logistic regression using **Hessian** method. This class is designed to work for datasets having binary classes. This class will be further used for datasets with multi-class\n",
    "\n",
    "As this class inherits most of it's features from **BinaryLogisticRegression** class\n",
    "We only modify what's needed to optimize it using Hessian method.\n",
    "\n",
    "Gradient is once again modified in this method to calculate hessian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS Binary Logistic Regression\n",
    "\n",
    "This class defines the optimization technique we can use for logistic regression using **quasi-Newton** methods is known as Broyden-Fletcher-Goldfarb-Shanno (BFGS). This class is designed to work for datasets having binary classes. This class will be further used for datasets with multi-class\n",
    "\n",
    "As this class inherits most of it's features from **BinaryLogisticRegression** class\n",
    "We only modify what's needed to optimize it using Hessian method.\n",
    "\n",
    "for this, we won't perform our own BFGS implementation since it takes a good deal of code and understanding of the algorithm\n",
    "luckily for us, scipy has its own BFGS implementation **fmin_bfgs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*np.sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g)) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Logistic Regression\n",
    "\n",
    "Here is the class which uses all of the above classes. The above classes was designed for datasets with binary classes. This class is class which will implement optimized logistic regression for multi-class datasets.\n",
    "\n",
    "The way to do it to implement binary methods for each unique class of the datasets using **one-versus-all** method.\n",
    "\n",
    "**note:** this class is modified from the source to include an extra parameter for the class called solver (optimization Technique)\n",
    "\n",
    "by adding this parameter we make this class generic to implement all types of optimized techniques by just providing the technique name in the solver prameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, solver=\"\"):\n",
    "        self.optimizationTechnique = solver\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            # According the optimization Technique provided, the method will select the appropriate technique to implement.\n",
    "            if self.optimizationTechnique == \"steepest descent\":\n",
    "                hblr = LineSearchLogisticRegression(self.eta,self.iters,self.C)\n",
    "            elif self.optimizationTechnique == \"stochastic gradient descent\":\n",
    "                hblr = StochasticLogisticRegression(self.eta,self.iters,self.C)\n",
    "            elif self.optimizationTechnique == \"Newtons method\":\n",
    "                hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            else :\n",
    "                hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            \n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        print(\"Using \"+self.optimizationTechnique+\" optimization technique\")\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Stochastic Logistic Regression for multi class\n",
    "\n",
    "the following code implements the Stochastic method for multi-class datasets.we add the parameter 'solver' which tells the MultiClassLogisticRegression class which technique to use. In this case it is stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = MultiClassLogisticRegression(eta=0.01,iterations=250,C=0.02,solver=\"stochastic gradient descent\")\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple tests we conclude the general performance for Multi-Class Logistic Regression using stochastic gradient descent\n",
    "\n",
    "**Accuracy** = 41.6833%\n",
    "\n",
    "**Wall time** = 28.4ms\n",
    "\n",
    "## Implementing Newton's method of Logistic Regression for multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = MultiClassLogisticRegression(eta=0.01,iterations=1,C=0.01,solver=\"Newtons method\")\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple tests we conclude the general performance for Multi-Class Logistic Regression using Newton's method.\n",
    "\n",
    "**Accuracy** = 43.38677%\n",
    "\n",
    "**Wall time** = 21.8ms\n",
    "\n",
    "## Implementing Steepest descent Logistic Regression for multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=100,C=0.01,solver=\"steepest descent\")\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple tests we conclude the general performance for Multi-Class Logistic Regression using steepest descent method.\n",
    "\n",
    "**Accuracy** = 43.38677%\n",
    "\n",
    "**Wall time** = 15.5ms\n",
    "\n",
    "## Implementing BFGS Logistic Regression for multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(_,iterations=8,C=0.00001,solver=\"default : BFGSBinaryLogisticRegression\")\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple tests we conclude the general performance for Multi-Class Logistic Regression using BFGS Binary Logistic Regression method. This is the most optimized method we found till now.\n",
    "\n",
    "**Accuracy** = 44.5891%\n",
    "\n",
    "**Wall time** = 9.93ms\n",
    "\n",
    "## Implementing Parallel Multi-Class Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# its still faster! Can we fix that?\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class ParallelMultiClassLogisticRegression(MultiClassLogisticRegression):\n",
    "    @staticmethod\n",
    "    def par_logistic(yval,eta,iters,C,X,y):\n",
    "        y_binary = y==yval # create a binary problem\n",
    "        # train the binary classifier for this class\n",
    "        hblr = BFGSBinaryLogisticRegression(eta,iters,C)\n",
    "        hblr.fit(X,y_binary)\n",
    "        return hblr\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        backend = 'threading' #'multiprocessing'\n",
    "        \n",
    "        self.classifiers_ = Parallel(n_jobs=-1,backend=backend)(\n",
    "            delayed(self.par_logistic)(yval,self.eta,self.iters,self.C,X,y) for yval in self.unique_)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "plr = ParallelMultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "plr.fit(X,y)\n",
    "print(plr)\n",
    "\n",
    "yhat = plr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple tests we conclude the general performance for Multi-Class Logistic Regression using parallel computing method.\n",
    "\n",
    "**Accuracy** = 43.8877%\n",
    "\n",
    "**Wall time** = 111ms\n",
    "\n",
    "\n",
    "by this we tried out all optimization techniques for logestic regression.\n",
    "\n",
    "# Implementing SKLearn for Grade prediction Dataset\n",
    "\n",
    "Now, the following code implements logistic regression with inbuilt optimization of SKLearn which is considered the best because the library uses all the optimization possible implicitly and provides with best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "\n",
    "lr_sk.fit(X,y)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, SKLearn implimentation provides the fastest Wall time and accuracy level among the highest.\n",
    "\n",
    "\n",
    "**Accuracy** = 43.6873%\n",
    "\n",
    "**Wall time** = 6.4 ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Regularization for Gradient Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classifier's Performance Based on Parameters Investigated\n",
    "\n",
    "After implementing the above optimization techniques for logistic regression, we found that BFGS is most suitable for our grade prediction model. So we decided to visualize the performance of this classifier and optimizer i.e it's accuracy against parameters which are regulation term and number of iterations. We executed the classifier by tuning the parameter values and collected about 25 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref - https://stackoverflow.com/questions/39715601/how-to-create-a-line-chart-using-matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_obv = pd.read_csv(r'D:\\Downloads\\Notes\\CSE_7324_ML\\Lab 3\\observations.csv') \n",
    "\n",
    "\n",
    "df_obv.head()\n",
    "\n",
    "C = df_obv['C'].values\n",
    "itrn = df_obv['Iterations'].values\n",
    "accr = df_obv['Accuracy'].values\n",
    "wt = df_obv['Walltime'].values\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(itrn, accr, label='Component 1', color='c', marker='o')\n",
    "ax1.grid(True)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy in percent')\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(C, accr, label='Component 1', color='r', marker='x')\n",
    "ax1.grid(True)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy in percent')\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(itrn, wt, label='Component 1', color='g', marker='o')\n",
    "ax1.grid(True)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Wall time')\n",
    "plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above 3 graph we can have serveral conclusions. In our classifier we observed that as we increase the number of iterations the accuracy level keeps increasing at a certain point. Iteration at this point is 8. After this the accuracy level fluctuate around tht range as we further increase the iterations. SO we kept the iteration number for our chosen classifier as 8.\n",
    "\n",
    "In the second graph we see accuracy level all scattered around the graph, this indicates that the dependancy of accuracy & time is not that consistant with the regulation term 'c'. After multiple observations we decided to move forward with 'c' value as 0.00001.\n",
    "\n",
    "The 3rd graph shows us that wall time is directly proportional to the number of iterations. SO this comes down to the requiement of the model as what is more important to us Accuracy or the speed. \n",
    "\n",
    "In the grade prediction model, we would consider accuracy to be of more priority over Wall time.\n",
    "\n",
    "\n",
    "## 2.3 Comparing Optimization Techniques\n",
    "\n",
    "From all the above multiclass logistic regression we performed the results can be summarized into the following table.\n",
    "\n",
    "<h2 style='padding: 10px'>Summary\n",
    "</h2><table class='table table-striped'> <thead> \n",
    "<tr> \n",
    "<th><u>Optimization Techniques</u></th> \n",
    "<th><u>Accuracy percent</u></th> \n",
    "<th><u>Wall time in ms</u></th> \n",
    "</tr> \n",
    "</thead> \n",
    "<tbody> \n",
    "<tr> <th scope='row'>Vector Multi-class Logistic Regression</th> <td>43.687374</td> <td>31.4</td> </tr>\n",
    "<tr> <th scope='row'>Stochastic gradient descent</th> <td>41.6833</td> <td>28.4</td> </tr> \n",
    "<tr> <th scope='row'>Newton's method</th> <td>43.38677</td> <td>21.8</td> </tr> \n",
    "<tr> <th scope='row'>Steepest descent</th> <td>43.38677</td> <td>15.5</td> </tr> \n",
    "<tr> <th scope='row'>BFGS Logistic Regression</th> <td>44.5891</td> <td>9.93</td> </tr> \n",
    "<tr> <th scope='row'>Parallel computing</th> <td>43.8877</td> <td>111</td> </tr> \n",
    "<tr> <th scope='row'>SK Logistic Regression</th> <td>43.6873</td> <td>6.4</td> </tr> \n",
    "</tbody> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now looking at the table, let's analyse the results. For the analysis, we are using scatter plot where the X axis is accuracy percent and Y axis is wall time in milliseconds of the various optimization techniques we used for logistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npacc = np.asarray([43.68, 41.68, 43.38, 43.38, 44.58, 43.88, 43.68])\n",
    "npwt = np.asarray([31.4, 28.4, 21.8, 15.5, 9.93, 111, 6.4])\n",
    "nplabels = np.asarray(['Vector Multi-class Logistic Regression', 'Stochastic gradient descent', 'Newton''s method', 'Steepest descent', 'BFGS Logistic Regression', 'Parallel computing', 'SK Logistic Regression'])\n",
    "\n",
    "colors = ['blue', 'cyan', 'orange', 'magenta', 'red', 'green', 'black']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "i = 0\n",
    "for color in ['blue', 'cyan', 'orange', 'magenta', 'red', 'green', 'black']:\n",
    "    scale = 200\n",
    "    ax.scatter(npwt[i], npacc[i], c=colors[i], s=scale, label=nplabels[i], alpha=0.5)\n",
    "    i=i+1\n",
    "\n",
    "ax.legend(prop={'size': 12},bbox_to_anchor=(3.0,0.75))\n",
    "ax.grid(True)\n",
    "plt.xlabel(\"Wall Time in ms\")\n",
    "plt.ylabel(\"Accuracy Percent\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From the graph, we can see that **Sci-kit learn** is the fastest compared to  all our implementations with walltime of 6.4 ms and gives the accuracy of 43.68%. Our best optimization which turns out to be **BFGS Logistic Regression** gives us the highest accuracy percent compared to all our implementations and sci-kit learn. The walltime is slightly more than sci-kit learn at 9.93 ms.\n",
    "\n",
    "Stochastic Gradient Descent is directly proportional to the number of iterations which also increases the walltime. So in our case with 250 iterations we get least accuracy here. However we can increase this accuracy at the expense of wall time. This technique is not recommended for our model since we already found a technique with better accuracy and time.\n",
    "\n",
    "Parallel Computing was found to take the most wall time i.e 111 ms. Generally parallel computing is most suitable for large number of features and unique classes. Our data has comparativley has only 3 features and works better with sequential computing techniques.\n",
    "\n",
    "Rest all the other techniques were found to give more or less same accuracy with slight difference in wall time.\n",
    "\n",
    "***Note:** Please note that these results have been noted from multiple observations and is generally found consistent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deployment\n",
    "\n",
    "From the conclusions we made It is clear, that choice of implementation for a machibe learning model is based upon the requirement. If we require more accuracy, we should go with **BFGS Logistic Regression** as it provides the best accuracy with reasonable amount of time.\n",
    "\n",
    "**Sci-kit Learn** can be used if observations are very large and need to have a smaller wall time. Sci-kit learn would server a good choice for those type of models.\n",
    "\n",
    "**Parallel computing** also provides a high accuracy, but it's ideal for dataset with many features or unique classes. The benefits of parallel computing may be seen only then.\n",
    "\n",
    "One can argue that sci-kit learn can internally also use paralle computing along with other optimization tricks to provide the best result. In order to conclude that multiple implementations will have to be done with different variety of machne learning models.\n",
    "\n",
    "In our case i.e Grade prediction model, We would go with BFGS Logistic Regression to take full advantage of the accuracy level provided. \n",
    "\n",
    "Looking at 44.6 % accuracy doesn't seem a lot, this is because of the limited observation we have. as per our Classifier's Performance Based on Parameters Investigated on section 2.2 we can say that as faculty members will go on adding more observations through time, Accuracy of the model will keep increasing.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
