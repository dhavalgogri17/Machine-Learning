{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "1. Piyush Metkar - 47509180\n",
    "2. Rishab Vaishya - 47505527\n",
    "3. Dhaval Gogri - 47444609\n",
    "\n",
    "# 1 BUSINESS UNDERSTANDING\n",
    "\n",
    "We are always surrounded by objects, wherever we go, which can be categorized into cars, baseball-bats, chairs, lamps etc. Nowadays with taking photos on mobile becoming a common phenomenon, image processing to extract data from images  has become a field of interests among many companies such as Google, Facebook, Amazon etc for <b>tagging</b> objects in images.\n",
    "\n",
    "Our data consists of 45 categories of objects with a total of more 5000 images.\n",
    "\n",
    "We have taken a data set of images which consists of many categories of objects. By visualizing our data set we can extract the features of data from all the train images we have and can use those features to identify that object in  test images containing multiple objects. For example, if you are selling a product on Amazon and you upload wrong images for your product, like photos of chair instead of lamp, our algorithm with help to recognize this mistake. It would compare the data we have collected and the products that match as the same way on Amazon. This will help in improving user experience as any user looking for a lamp won't have to go through chairs instead.\n",
    "\n",
    "Also it would he helpful in <b>tagging</b> objects in images. For example an image of a person with a car and an apple in hand. It would then recognize the person, car and the apple in the image.\n",
    "\n",
    "For our algorithm to be successful, it has to clearly identify atleast the objects which are visually very different. Feature extraction techniques should be able to highlight the main components of images and distinguish them. It is ok to miss few items which are visually similar. For example, the canopy of an umbrella and the shade of lamps are visually similar and hence we expect to miss such items. Overall we are looking at <b>90%</b> hit rate.\n",
    "\n",
    "\n",
    "# 2. DATA PREPARATION\n",
    "\n",
    "We will resize all the images to 100x100 resolution for maintaining consistency. Also we will perform grey-scale conversion on all the images by removing all the RGB and alpha layer.\n",
    "\n",
    "We have linearized the array into a 1-D array as per the requirement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 294\n",
      "Target: 294\n",
      "FileNames: 294\n",
      "TargetNames: 5\n",
      "X Shape- (5485, 10000)\n",
      "y Shape- (294,)\n",
      "255.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets as ds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "low_memory = False\n",
    "\n",
    "#Read in images into dataset object \n",
    "\n",
    "imageFolderPath=r'/Users/piyushmetkar/101_Object'\n",
    "im_array = ds.load_files(imageFolderPath, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0) \n",
    "\n",
    "#Here ds.loadfiles() reads the images as text files. So we use matplotlib.image to read the image file\n",
    "#ds.loadfiles() gives us an object of the entire dataset and maps filenames, target, targetnames accordingly\n",
    "\n",
    "print('Data:',len(im_array.data))\n",
    "print('Target:',len(im_array.target))\n",
    "print('FileNames:',len(im_array.filenames))\n",
    "print('TargetNames:',len(im_array.target_names))\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "im_array.data = np.zeros((5485, 10000))\n",
    "\n",
    "for i in range(len(im_array.data)):\n",
    "    try:\n",
    "        img = mpimg.imread(im_array.filenames[i]) #Using imread() to read image files\n",
    "        #img.resize(100,100,100) # resizing all images to 50, 50\n",
    "        gray = rgb2gray(img)    # Converting all images to grayscale\n",
    "        farr = gray.flatten()   # Linearize the images to 1-D image features\n",
    "        im_array.data[i] = farr # replace in corresponding data\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "X = np.array(im_array.data)\n",
    "y = np.array(im_array.target)\n",
    "names = im_array.target_names\n",
    "print('X Shape-', X.shape)\n",
    "print('y Shape-', y.shape)\n",
    "\n",
    "print(X[0][0])\n",
    "n_samples, n_features = X.shape\n",
    "h, w = 100,100\n",
    "#n_classes = len(names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4e92302c66d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# add in the class targets and names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2586\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2760\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3121\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(im_array.data)\n",
    "# add in the class targets and names\n",
    "df['Target'] = im_array.target.astype(np.int)\n",
    "print (df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 DATA REDUCTION\n",
    "\n",
    "## 3.1 Linear dimensionality reduction of the images using principal components analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit(X).transform(X) # fit data and then transform it\n",
    "\n",
    "# print the components\n",
    "print ('pca:', pca.components_.shape)\n",
    "\n",
    "def plot_explained_variance(pca):\n",
    "    import plotly\n",
    "    from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis, Bar, Line\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "    \n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })\n",
    "\n",
    "pca = PCA(n_components=200)\n",
    "X_pca = pca.fit(X)\n",
    "plot_explained_variance(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus here we need about <b><u>59</u></b> principal components to adequately represent 90% of the image data.\n",
    "\n",
    "## 3.2  Non-linear dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "n_components = 10\n",
    "print (\"Extracting the top %d eigenobjects from %d objects, not calculating inverse transform\" % (n_components, X.shape[0]))\n",
    "\n",
    "kpca = KernelPCA(n_components=n_components, kernel='rbf', \n",
    "                fit_inverse_transform=False, gamma=8, # very sensitive to the gamma parameter,\n",
    "                remove_zero_eig=True)  \n",
    "kpca.fit(X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#  THIS  TAKES A LONG TIME TO RUN\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "n_components = 10\n",
    "print (\"Extracting the top %d eigenobjects from %d objects, ALSO getting inverse transform\" % (n_components, X.shape[0]))\n",
    "\n",
    "kpca = KernelPCA(n_components=n_components, kernel='rbf', \n",
    "                fit_inverse_transform=True, gamma=8, # very sensitive to the gamma parameter,\n",
    "                remove_zero_eig=True)  \n",
    "kpca.fit(X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the above operation takes a long time to save the inverse transform parameters\n",
    "# so let's save out the results to load in later!\n",
    "import pickle\n",
    "\n",
    "pickle.dump(kpca, open( '/Users/piyushmetkar/Desktop/KPCA/kpca.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Kpca\n",
    "import pickle\n",
    "kpca = pickle.load(open( '/Users/piyushmetkar/Desktop/KPCA/kpca.p', 'rb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widgets example\n",
    "from ipywidgets import widgets  # make this interactive!\n",
    "def f(x):\n",
    "    return x\n",
    "widgets.interact(f, x=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.simplefilter('ignore', DeprecationWarning)\n",
    "# warnings.simplefilter(\"always\",DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "def plt_reconstruct(idx_to_reconstruct):\n",
    "    idx_to_reconstruct = np.round(idx_to_reconstruct)\n",
    "    \n",
    "    reconstructed_image = pca.inverse_transform(pca.transform(X[idx_to_reconstruct].reshape(1, -1)))\n",
    "    #reconstructed_image_rpca = rpca.inverse_transform(rpca.transform(X[idx_to_reconstruct].reshape(1, -1)))\n",
    "    reconstructed_image_kpca = kpca.inverse_transform(kpca.transform(X[idx_to_reconstruct].reshape(1, -1)))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    imshow(X[idx_to_reconstruct].reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title(names[y[idx_to_reconstruct]])\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    imshow(reconstructed_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Full PCA')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    imshow(reconstructed_image_kpca.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Kernel PCA')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "widgets.interact(plt_reconstruct,idx_to_reconstruct=(0,n_samples-1,1),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 \n",
    "When we compare PCA with Kernel PCA, it is observed that we get higher principal components for KPCA when the number of images are less. However, as number of images increases, the KPCA becomes very smudgy. \n",
    "\n",
    "On an average standard PCA yeilds approx 40 more prinicpal components than KPCA when less images are fed in which they are visually similar. Thus PCA is better at dimensionality reduction.\n",
    "\n",
    "\n",
    "## 3.4 Feature Extraction - Gabor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from ipywidgets import fixed\n",
    "# put it together inside a nice widget\n",
    "def closest_image(dmat,idx1):\n",
    "    distances = copy.deepcopy(dmat[idx1,:]) # get all image diatances\n",
    "    distances[idx1] = np.infty # dont pick the same image!\n",
    "    idx2 = np.argmin(distances)\n",
    "    \n",
    "    distances[idx2] = np.infty\n",
    "    idx3 = np.argmin(distances)\n",
    "    \n",
    "    plt.figure(figsize=(10,16))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(X[idx1].reshape((h,w)))\n",
    "    plt.title(\"Original Image \"+names[y[idx1]])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(X[idx2].reshape((h,w)))\n",
    "    plt.title(\"Closest Image  \"+names[y[idx2]])\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(X[idx3].reshape((h,w)))\n",
    "    plt.title(\"Next Closest Image \"+names[y[idx3]])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "#widgets.interact(closest_image,idx1=(0,n_samples-1,1),dmat=fixed(dist_matrix),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "from scipy import stats\n",
    "\n",
    "# prepare filter bank kernels\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,\n",
    "                                          sigma_x=sigma, sigma_y=sigma))\n",
    "            kernels.append(kernel)\n",
    "\n",
    "            \n",
    "# compute the filter bank and take statistics of image\n",
    "def compute_gabor(row, kernels, shape):\n",
    "    feats = np.zeros((len(kernels), 4), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(row.reshape(shape), kernel, mode='wrap')\n",
    "        _,_,feats[k,0],feats[k,1],feats[k,2],feats[k,3] = stats.describe(filtered.reshape(-1))\n",
    "        # mean, var, skew, kurt\n",
    "        \n",
    "    return feats.reshape(-1)\n",
    "\n",
    "idx_to_reconstruct = int(np.random.rand(1)*len(X))\n",
    "\n",
    "gabr_feature = compute_gabor(X[idx_to_reconstruct], kernels, (h,w))\n",
    "gabr_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~3 minutes to run entire dataset\n",
    "%time gabor_stats = np.apply_along_axis(compute_gabor, 1, X, kernels, (h,w))\n",
    "print(gabor_stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# find the pairwise distance between all the different image features\n",
    "%time dist_matrix_gabor = pairwise_distances(gabor_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.show()\n",
    "widgets.interact(closest_image,idx1=(0,n_samples-1,1),dmat=fixed(dist_matrix_gabor),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5\n",
    "As we can see above, Gabor works very well for our business case. In approx 90% of the cases, it identifies the closest neighbour from the same target class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
